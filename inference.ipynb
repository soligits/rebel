{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d09f27ad-f000-42ff-b15f-85484d4cbff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/rebel/src\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/rebel/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64658ad6-b1e2-4bef-99ff-b729fe6f6eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/rebel/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from hydra import compose, initialize\n",
    "import pytorch_lightning as pl\n",
    "from pl_data_modules import BasePLDataModule\n",
    "from pl_modules import BasePLModule\n",
    "from transformers import AutoConfig, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from pytorch_lightning.core.saving import _load_from_checkpoint\n",
    "from typing import cast\n",
    "import torch\n",
    "from pytorch_lightning.utilities.migration.utils import _pl_migrate_checkpoint\n",
    "from pathlib import Path\n",
    "from pytorch_lightning.core.saving import _load_state\n",
    "\n",
    "checkpoint_path = \"/home/ubuntu/rebel/outputs/2024-08-21/17-25-14/experiments/default_name/last.ckpt\"\n",
    "seed = 42\n",
    "config_name = 'google/mt5-base'\n",
    "tokenizer_name = 'google/mt5-base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e997d2e-9ff4-4122-838b-c1a3aa87ddbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_725720/2836277268.py:1: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  initialize(config_path=\"./conf\")\n"
     ]
    }
   ],
   "source": [
    "initialize(config_path=\"./conf\")\n",
    "conf = compose(config_name=\"root\", overrides=[f\"checkpoint_path={checkpoint_path}\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66965ba1-ece3-47e5-ac7f-387194950517",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/rebel/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/home/ubuntu/miniconda3/envs/rebel/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MT5ForConditionalGeneration(\n",
       "  (shared): Embedding(250112, 768)\n",
       "  (encoder): MT5Stack(\n",
       "    (embed_tokens): Embedding(250112, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): MT5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): MT5Stack(\n",
       "    (embed_tokens): Embedding(250112, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerCrossAttention(\n",
       "            (EncDecAttention): MT5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerCrossAttention(\n",
       "            (EncDecAttention): MT5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): MT5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=250112, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(\n",
    "    conf.config_name if conf.config_name else conf.model_name_or_path,\n",
    "    decoder_start_token_id = 0,\n",
    "    early_stopping = False,\n",
    "    no_repeat_ngram_size = 0,\n",
    ")\n",
    "\n",
    "special_tokens = [\n",
    "    \"<triplet>\",\n",
    "    \"<obj>\",\n",
    "    \"<subj>\",\n",
    "]\n",
    "    \n",
    "tokenizer_kwargs = {\n",
    "    \"use_fast\": True,  # Always use fast tokenizer for better compatibility\n",
    "    \"additional_special_tokens\": special_tokens, \n",
    "    \"legacy\": False,  # For mt5\n",
    "}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    conf.tokenizer_name if conf.tokenizer_name else conf.model_name_or_path,\n",
    "    **tokenizer_kwargs\n",
    ")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    conf.model_name_or_path,\n",
    "    config=config,\n",
    ")\n",
    "# if not conf.finetune:\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7485e86-b3c1-40e3-b2ae-daa4557d78af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # data module declaration\n",
    "# pl_data_module = BasePLDataModule(conf, tokenizer, model)\n",
    "\n",
    "# # main module declaration\n",
    "# pl_module = BasePLModule(conf, config, tokenizer, model)\n",
    "# device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc2fa0e9-ca4c-4421-8d72-54dabbf62da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_725720/1169818071.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for MT5ForConditionalGeneration:\n\tsize mismatch for shared.weight: copying a param with shape torch.Size([250103, 768]) from checkpoint, the shape in current model is torch.Size([250112, 768]).\n\tsize mismatch for encoder.embed_tokens.weight: copying a param with shape torch.Size([250103, 768]) from checkpoint, the shape in current model is torch.Size([250112, 768]).\n\tsize mismatch for decoder.embed_tokens.weight: copying a param with shape torch.Size([250103, 768]) from checkpoint, the shape in current model is torch.Size([250112, 768]).\n\tsize mismatch for lm_head.weight: copying a param with shape torch.Size([250103, 768]) from checkpoint, the shape in current model is torch.Size([250112, 768]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 12\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# checkpoint = _pl_migrate_checkpoint(\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#     checkpoint, checkpoint_path=(conf.checkpoint_path if isinstance(conf.checkpoint_path, (str, Path)) else None)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# model_state_dict = \u001b[39;00m\n\u001b[1;32m      9\u001b[0m \n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Remove the 'model.' prefix from the keys if present\u001b[39;00m\n\u001b[1;32m     11\u001b[0m model_state_dict \u001b[38;5;241m=\u001b[39m {k\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel.\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m): v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate_dict\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m---> 12\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_state_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# model = model.to(torch.device('cuda:0'))\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rebel/lib/python3.10/site-packages/torch/nn/modules/module.py:2215\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2210\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2211\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2212\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2216\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for MT5ForConditionalGeneration:\n\tsize mismatch for shared.weight: copying a param with shape torch.Size([250103, 768]) from checkpoint, the shape in current model is torch.Size([250112, 768]).\n\tsize mismatch for encoder.embed_tokens.weight: copying a param with shape torch.Size([250103, 768]) from checkpoint, the shape in current model is torch.Size([250112, 768]).\n\tsize mismatch for decoder.embed_tokens.weight: copying a param with shape torch.Size([250103, 768]) from checkpoint, the shape in current model is torch.Size([250112, 768]).\n\tsize mismatch for lm_head.weight: copying a param with shape torch.Size([250103, 768]) from checkpoint, the shape in current model is torch.Size([250112, 768])."
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load(\n",
    "    conf.checkpoint_path,\n",
    "    # map_location=torch.device('cpu'),\n",
    ")\n",
    "# checkpoint = _pl_migrate_checkpoint(\n",
    "#     checkpoint, checkpoint_path=(conf.checkpoint_path if isinstance(conf.checkpoint_path, (str, Path)) else None)\n",
    "# )\n",
    "# model_state_dict = \n",
    "\n",
    "# Remove the 'model.' prefix from the keys if present\n",
    "model_state_dict = {k.replace('model.', ''): v for k, v in checkpoint['state_dict'].items()}\n",
    "model.load_state_dict(model_state_dict)\n",
    "# model = model.to(torch.device('cuda:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "670ac2a2-8e87-441b-a0e8-43420f0865af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MT5ForConditionalGeneration(\n",
       "  (shared): Embedding(250103, 768)\n",
       "  (encoder): MT5Stack(\n",
       "    (embed_tokens): Embedding(250103, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): MT5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): MT5Stack(\n",
       "    (embed_tokens): Embedding(250103, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerCrossAttention(\n",
       "            (EncDecAttention): MT5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerCrossAttention(\n",
       "            (EncDecAttention): MT5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): MT5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=250103, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ac854b8-3f12-4925-8241-3caa261a8385",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_triplets(text):\n",
    "    triplets = []\n",
    "    relation, subject, relation, object_ = '', '', '', ''\n",
    "    text = (\n",
    "        text\n",
    "        .replace(\"<obj>\", \" <obj> \")\n",
    "        .replace(\"<subj>\", \" <subj> \")\n",
    "        .replace(\"<triplet>\", \" <triplet> \")\n",
    "        .replace(\"<s>\", \"\")\n",
    "        .replace(\"</s>\", \"\")\n",
    "        .replace(\"<pad>\", \"\")\n",
    "        .strip()\n",
    "    )\n",
    "    for i in range(100):\n",
    "        text = text.replace(f\"<extra_id_{i}>\", \"\")\n",
    "    current = 'x'\n",
    "    for token in text.split():\n",
    "        token = token.strip()\n",
    "        if token == \"<triplet>\":\n",
    "            current = 't'\n",
    "            if relation != '':\n",
    "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "                relation = ''\n",
    "            subject = ''\n",
    "        elif token == \"<subj>\":\n",
    "            current = 's'\n",
    "            if relation != '':\n",
    "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "            object_ = ''\n",
    "        elif token == \"<obj>\":\n",
    "            current = 'o'\n",
    "            relation = ''\n",
    "        else:\n",
    "            if current == 't':\n",
    "                subject += ' ' + token\n",
    "            elif current == 's':\n",
    "                object_ += ' ' + token\n",
    "            elif current == 'o':\n",
    "                relation += ' ' + token\n",
    "    if subject.strip() and relation.strip() and object_.strip():\n",
    "        triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "    triplets = [item for item in triplets if item[\"head\"] and item[\"type\"] and item[\"tail\"]]\n",
    "    return triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7a6c573-729b-4a66-b201-01cef2c4e627",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _predict(model, tokenizer, text):\n",
    "    gen_kwargs = {\n",
    "        \"max_length\": 128,\n",
    "        \"early_stopping\": False,\n",
    "        \"length_penalty\": 0,\n",
    "        \"no_repeat_ngram_size\": 0,\n",
    "        \"num_beams\": 3,\n",
    "    }\n",
    "    \n",
    "    model_inputs = tokenizer(text, max_length=256, padding=True, truncation=True, return_tensors = 'pt')\n",
    "    batch_size = model_inputs['input_ids'].shape[0]\n",
    "    decoder_inputs = torch.tensor([[0, 250100] for _ in range(batch_size)])\n",
    "    generated_tokens = model.generate(\n",
    "        model_inputs[\"input_ids\"].to(model.device),\n",
    "        attention_mask=model_inputs[\"attention_mask\"].to(model.device),\n",
    "        decoder_input_ids=decoder_inputs.to(model.device),\n",
    "        **gen_kwargs,\n",
    "    )\n",
    "    \n",
    "    decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=False)\n",
    "    return decoded_preds\n",
    "\n",
    "predict = lambda text: _predict(model, tokenizer, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2eb292cd-4790-4621-98bd-2f6d2d111d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "اینشتین در ۱۴ مارس ۱۸۷۹ در اولم آلمان به دنیا آمد. خانواده‌اش یهودی بودند و در ۱۸۸۰ به مونیخ مهاجرت کردند. پدرش هرمان یک فروشنده و مهندس بود و عموی او یاکوب با او تجارت کوچکی برای نصب گاز و آب تأسیس کردند. این تجارت موفق بود و در ۱۸۸۵ کارخانه تولید لوازم الکتریکی راه‌اندازی کردند.\n",
    "\n",
    "اینشتین در ۵ سالگی به مدرسه ابتدایی کاتولیک در مونیخ رفت و سه سال بعد به مدرسه لوتیپولد فرستاده شد. او در ۱۴ سالگی به ایتالیا رفت و در پاویا ساکن شد. پدرش تمایل داشت که او رشته مهندسی برق را ادامه دهد اما اینشتین به‌خاطر برخوردهایی که با مسئولان مدرسه داشت از آن‌ها متنفر شده بود.\n",
    "\n",
    "اینشتین حتی از دوران جوانی تبحر خاصی در ریاضی و فیزیک داشت. او از ۱۲ سالگی شروع به خودآموزی جبر و هندسه اقلیدسی کرد و در سن ۱۲ سالگی اثباتی اختصاصی برای قضیه فیثاغورس بیابد. او در سن ۱۴ سالگی به گفته خودش در انتگرال و حساب دیفرانسیل به استادی رسیده بود.\n",
    "\n",
    "اینشتین در ۱۸۹۴ کارخانه پدرش را ترک کرد و به ایتالیا رفت. او در پاویا ساکن شد و در ۱۸۹۵ به دانشگاه لا ساپیenza در رم رفت. او در آنجا با ماکس پلانک آشنا شد و تحت تأثیر او قرار گرفت.\n",
    "\n",
    "\"\"\"\n",
    "sentences = [(x.strip() + \".\") for item in text.strip().split(\"\\n\") for x in item.strip().split(\".\") if x.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ec4e6302-ed54-4888-96bd-b6ba21189197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: اینشتین در ۱۴ مارس ۱۸۷۹ در اولم آلمان به دنیا آمد., output: {'head': 'اولم', 'type': 'کشور', 'tail': 'آلمان'}\n",
      "input: خانواده‌اش یهودی بودند و در ۱۸۸۰ به مونیخ مهاجرت کردند., output: {'head': 'مونیخ', 'type': 'کشور', 'tail': 'یهودی'}\n",
      "input: پدرش هرمان یک فروشنده و مهندس بود و عموی او یاکوب با او تجارت کوچکی برای نصب گاز و آب تأسیس کردند., output: {'head': 'هرمان', 'type': 'همنیا', 'tail': 'یاکوب'}\n",
      "input: این تجارت موفق بود و در ۱۸۸۵ کارخانه تولید لوازم الکتریکی راه‌اندازی کردند., output: {'head': 'لوازم الکتریکی', 'type': 'زیرردۀ', 'tail': 'لوازم الکتریکی'}\n",
      "input: اینشتین در ۵ سالگی به مدرسه ابتدایی کاتولیک در مونیخ رفت و سه سال بعد به مدرسه لوتیپولد فرستاده شد., output: {'head': 'مدرسه ابتدایی کاتولیک', 'type': 'موقعیت در تقسیمات کشوری', 'tail': 'مونیخ'}\n",
      "input: او در ۱۴ سالگی به ایتالیا رفت و در پاویا ساکن شد., output: {'head': 'پاویا', 'type': 'کشور', 'tail': 'ایتالیا'}\n",
      "input: پدرش تمایل داشت که او رشته مهندسی برق را ادامه دهد اما اینشتین به‌خاطر برخوردهایی که با مسئولان مدرسه داشت از آن‌ها متنفر شده بود., output: {'head': 'اینشتین', 'type': 'زمینه کاری', 'tail': 'مهندسی برق'}\n",
      "input: اینشتین حتی از دوران جوانی تبحر خاصی در ریاضی و فیزیک داشت., output: {'head': 'ریاضی', 'type': 'جزئی از', 'tail': 'فیزیک'}\n",
      "input: او از ۱۲ سالگی شروع به خودآموزی جبر و هندسه اقلیدسی کرد و در سن ۱۲ سالگی اثباتی اختصاصی برای قضیه فیثاغورس بیابد., output: {'head': 'فیثاغورس', 'type': 'جزئی از', 'tail': 'هندسه اقلیدسی'}\n",
      "input: او در سن ۱۴ سالگی به گفته خودش در انتگرال و حساب دیفرانسیل به استادی رسیده بود., output: {'head': 'انتگرال', 'type': 'متفاوت است با', 'tail': 'حساب دیفرانسیل'}\n",
      "input: اینشتین در ۱۸۹۴ کارخانه پدرش را ترک کرد و به ایتالیا رفت., output: {'head': 'ایتالیا', 'type': 'رابطۀ دیپلماتیک', 'tail': 'ایتالیا'}\n",
      "input: او در پاویا ساکن شد و در ۱۸۹۵ به دانشگاه لا ساپیenza در رم رفت., output: {'head': 'دانشگاه لا ساپیenza', 'type': 'موقعیت در تقسیمات کشوری', 'tail': 'رم'}\n",
      "input: او در آنجا با ماکس پلانک آشنا شد و تحت تأثیر او قرار گرفت., output: {'head': 'ماکس پلانک', 'type': 'همسر', 'tail': 'ماکس پلانک'}\n"
     ]
    }
   ],
   "source": [
    "for sen in sentences:\n",
    "    decoded_preds = predict(sen.strip())\n",
    "    print(f\"input: {sen}, output: {extract_triplets(decoded_preds[0])[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf9e9c4-8d19-4b62-aeb9-7857f18d7d74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
